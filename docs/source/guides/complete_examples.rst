Complete OpenHCS Examples
=========================

This guide provides complete, working examples that demonstrate all major OpenHCS functionality. These examples are perfect for agents, developers, and users who want to understand practical usage patterns.

üöÄ **Gold Standard Production Script**
--------------------------------------

**Location**: `openhcs/debug/example_export.py <https://github.com/trissim/toolong/blob/openhcs/openhcs/debug/example_export.py>`_

This is a **complete, production-ready script** generated by the OpenHCS TUI that demonstrates every major feature:

**What it demonstrates**:

‚úÖ **Complete Configuration System**:
- GlobalPipelineConfig with all sub-configurations
- VFS backend selection (memory + ZARR materialization)
- GPU resource management and parallel processing
- Production-grade compression and chunking strategies

‚úÖ **All Function Patterns**:
- **List Pattern**: Sequential function chains with parameters
- **Dictionary Pattern**: Channel-specific processing (DAPI vs GFP)
- **Single Functions**: Simple function calls
- **Parameterized Functions**: Real parameter values for production use

‚úÖ **Complete Workflow**:
- **Preprocessing**: Percentile normalization + morphological filtering
- **Composition**: Multi-channel composite creation
- **Stitching**: GPU-accelerated position finding + image assembly
- **Analysis**: Cell counting + neurite tracing (channel-specific)

‚úÖ **Production Features**:
- **100GB+ Dataset Handling**: ZARR compression with memory overlay
- **GPU Acceleration**: CuPy, PyTorch, GPU stitching algorithms
- **Multi-Backend Processing**: Automatic memory type conversion
- **Parallel Execution**: 5 workers with GPU scheduling

Key Code Patterns
------------------

**Complete Configuration**:

.. code-block:: python

    from openhcs.core.config import (GlobalPipelineConfig, PathPlanningConfig, 
                                     VFSConfig, ZarrConfig, MaterializationBackend, 
                                     ZarrCompressor, ZarrChunkStrategy)
    
    global_config = GlobalPipelineConfig(
        num_workers=5,
        path_planning=PathPlanningConfig(
            output_dir_suffix="_stitched",
            global_output_folder="/data/output/",
            materialization_results_path="results"
        ),
        vfs=VFSConfig(
            intermediate_backend=Backend.MEMORY,
            materialization_backend=MaterializationBackend.ZARR
        ),
        zarr=ZarrConfig(
            store_name="images.zarr",
            compressor=ZarrCompressor.ZSTD,
            compression_level=1,
            shuffle=True,
            chunk_strategy=ZarrChunkStrategy.SINGLE,
            ome_zarr_metadata=True,
            write_plate_metadata=True
        ),
        microscope=Microscope.AUTO
    )

**Function Chain Pattern**:

.. code-block:: python

    from openhcs.core.steps.function_step import FunctionStep
    from openhcs.processing.backends.processors.torch_processor import stack_percentile_normalize
    from openhcs.processing.backends.processors.cupy_processor import tophat
    
    # Sequential processing chain
    step = FunctionStep(
        func=[
            (stack_percentile_normalize, {
                'low_percentile': 1.0,
                'high_percentile': 99.0,
                'target_max': 65535.0
            }),
            (tophat, {
                'selem_radius': 50,
                'downsample_factor': 4
            })
        ],
        name="preprocess",
        variable_components=[VariableComponents.SITE],
        force_disk_output=False
    )

**Dictionary Pattern for Channel-Specific Analysis**:

.. code-block:: python

    from openhcs.processing.backends.analysis.cell_counting_cpu import count_cells_single_channel, DetectionMethod
    from openhcs.processing.backends.analysis.skan_axon_analysis import skan_axon_skeletonize_and_analyze, AnalysisDimension
    
    # Different analysis for different channels
    step = FunctionStep(
        func={
            '1': [  # DAPI channel - cell counting
                (count_cells_single_channel, {
                    'min_sigma': 1.0,
                    'max_sigma': 10.0,
                    'threshold': 0.1,
                    'detection_method': DetectionMethod.WATERSHED
                })
            ],
            '2': [  # GFP channel - neurite tracing
                (skan_axon_skeletonize_and_analyze, {
                    'min_branch_length': 10.0,
                    'analysis_dimension': AnalysisDimension.TWO_D
                })
            ]
        },
        name="analysis",
        variable_components=[VariableComponents.SITE]
    )

**GPU Stitching Workflow**:

.. code-block:: python

    from openhcs.processing.backends.pos_gen.ashlar_main_gpu import ashlar_compute_tile_positions_gpu
    from openhcs.processing.backends.assemblers.assemble_stack_cupy import assemble_stack_cupy
    
    # GPU position calculation
    positions_step = FunctionStep(
        func=[(ashlar_compute_tile_positions_gpu, {
            'overlap_ratio': 0.1,
            'max_shift': 15.0,
            'stitch_alpha': 0.2
        })],
        name="find_positions",
        variable_components=[VariableComponents.SITE],
        force_disk_output=False
    )
    
    # GPU image assembly
    assembly_step = FunctionStep(
        func=[(assemble_stack_cupy, {
            'blend_method': "fixed",
            'fixed_margin_ratio': 0.1
        })],
        name="assemble",
        variable_components=[VariableComponents.SITE],
        force_disk_output=True
    )

**Complete Orchestrator Execution**:

.. code-block:: python

    from openhcs.core.orchestrator.orchestrator import PipelineOrchestrator
    from openhcs.core.orchestrator.gpu_scheduler import setup_global_gpu_registry
    
    # Setup GPU registry
    os.environ["OPENHCS_SUBPROCESS_MODE"] = "1"
    setup_global_gpu_registry(global_config=global_config)
    
    # Execute pipeline
    orchestrator = PipelineOrchestrator(plate_path, global_config=global_config)
    orchestrator.initialize()
    compiled_contexts = orchestrator.compile_pipelines(pipeline_steps)
    orchestrator.execute_compiled_plate(
        pipeline_definition=pipeline_steps,
        compiled_contexts=compiled_contexts,
        max_workers=global_config.num_workers
    )

Running the Example
-------------------

**Prerequisites**:
- OpenHCS installed
- GPU with CUDA support (optional but recommended)
- Microscopy data in supported format

**Steps**:

.. code-block:: bash

    # Clone repository
    git clone https://github.com/trissim/toolong.git
    cd toolong
    
    # View the complete script
    cat openhcs/debug/example_export.py
    
    # Modify plate path for your data
    # Edit line 33: plate_paths = ['/path/to/your/plate']
    
    # Run the complete pipeline
    python openhcs/debug/example_export.py

**Expected Output**:
- Preprocessed images with normalization and filtering
- Multi-channel composite images
- Stitched high-resolution images
- Cell count analysis results
- Neurite tracing analysis results
- All outputs saved with ZARR compression

For Agents and Developers
-------------------------

**This example script is the perfect reference for**:

ü§ñ **AI Agents**: Complete, working patterns for all OpenHCS functionality
üîß **Developers**: Real parameter values and production configurations
üêõ **Debugging**: Working baseline to compare against
üìö **Learning**: Practical examples of every major feature
üöÄ **Production**: Battle-tested configuration and workflow patterns

**The script demonstrates**:
- Exact import paths that work
- Real parameter values for production use
- Complete configuration for 100GB+ datasets
- GPU acceleration patterns
- Error handling and signal management
- Production-grade logging and monitoring

See Also
--------

- :doc:`../getting_started/getting_started` - Getting started with the example
- :doc:`../api/orchestrator` - PipelineOrchestrator API reference
- :doc:`../api/function_step` - FunctionStep API reference
- :doc:`../api/config` - Configuration system reference
- :doc:`../concepts/function_handling` - Function pattern concepts
