.. _directory-structure:

===================
Directory Structure
===================

Overview
--------

OpenHCS employs a modern, **Virtual File System (VFS)**-first approach to data management. Unlike traditional file-based pipelines that create numerous intermediate directories and files on disk, OpenHCS primarily operates on data in memory. This significantly boosts performance by minimizing I/O overhead.

Physical directories are only created for two main purposes:

1.  **Initial Input:** The original, read-only microscopy data.
2.  **Final Materialization:** The final results of a pipeline, which are written to a persistent storage backend like Disk or Zarr.

.. note::
   The OpenHCS Virtual File System internally uses symbolic links (symlinks) to manage data efficiently, particularly for creating workspace views of the original data without copying files. For the `disk` backend to function correctly, your data should be located on a filesystem that supports symlinks (e.g., ext4, XFS, NTFS).

This document explains the simple and predictable directory structure governed by the global configuration.

Core Concepts
-------------

-  **Plate Path**: The path to the original, read-only microscopy data. OpenHCS will never modify the contents of this directory.
-  **Virtual File System (VFS)**: An in-memory representation of the filesystem where all intermediate processing occurs. The output of one step becomes the input to the next entirely within this memory space, avoiding disk access.
-  **Materialization**: The process of writing data from the in-memory VFS to a persistent storage backend (e.g., saving the final stitched images to a Zarr store).
-  **Symlinked Workspace**: Before processing, OpenHCS creates a "shadow" copy of your input directory structure using symbolic links. This provides a safe, isolated environment for the pipeline, ensuring original data is never modified.

Configuration-Driven Structure
--------------------------------

The entire output directory structure is controlled by the ``PathPlanningConfig`` object within your ``GlobalPipelineConfig``.

.. code-block:: python

   from openhcs.core.config import GlobalPipelineConfig, PathPlanningConfig

   # Example configuration to control output paths
   config = GlobalPipelineConfig(
       path_planning=PathPlanningConfig(
           global_output_folder="/path/to/my_hcs_results/",
           output_dir_suffix="_processed",
           materialization_results_path="analysis"
       )
   )

These parameters define the final output structure as follows:

1.  **`global_output_folder`**: This is the top-level directory where all outputs for all plates will be saved. If you process multiple plates, each will get its own subdirectory within this folder.
2.  **`output_dir_suffix`**: When the pipeline completes, the final materialized image data will be saved in a directory named `{plate_name}{output_dir_suffix}`.
3.  **`materialization_results_path`**: Any non-image results (like CSV files from analysis or other special outputs) will be saved in a subdirectory with this name.

Example Layout
--------------

Given the configuration above and an input plate named `plate1`, the final materialized output on disk would look like this:

.. code-block:: text

    # Input (read-only)
    /path/to/input_data/plate1/
    ├── Images/
    │   └── ... (image files)
    └── Metadata/
        └── ... (metadata files)

    # Output (generated by OpenHCS)
    /path/to/my_hcs_results/
    └── plate1_processed/              # {plate_name}{output_dir_suffix}
        ├── images.zarr/             # Final image data (if materialization backend is ZARR)
        └── analysis/                # {materialization_results_path}
            └── cell_counts.csv

Pipeline Flow and Workspaces
----------------------------

The flow of data between steps is managed internally by the VFS, using symlinks to create safe workspaces.

1.  **Workspace Creation**: When a pipeline is initialized, the `PipelineOrchestrator` uses the VFS to create a symlinked mirror of the input plate directory. This creates a safe, isolated "workspace" without duplicating large data files.
2.  **In-Memory Operations**: The first step reads data *through* the symlinked workspace into the in-memory VFS.
3.  **VFS Chaining**: Each subsequent step reads the output of the previous step from memory and writes its own output back to memory. No intermediate files are written to disk.
4.  **Final Materialization**: At the end of the pipeline, or when a step uses ``force_disk_output=True``, the final data is written from memory to the physical output directory defined by your ``PathPlanningConfig``.

This modern architecture makes pipeline definition extremely simple, as you no longer need to reason about or manage a complex chain of intermediate directories on disk.

**Handling Generic Directories (Disk Workspaces)**

While OpenHCS is optimized for structured microscopy plates, it can also process generic folders of images. If you provide a path to a simple directory instead of a plate, OpenHCS treats that directory's name as the "plate name" for creating output paths.

For example, if your `global_output_folder` is `/hcs/` and you process a generic folder named `/data/my_images/`, the output will be created at `/hcs/my_images_processed/`. The internal VFS and symlinked workspace workflow remains exactly the same.
